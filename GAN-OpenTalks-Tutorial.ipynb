{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративно-состязательная сеть. Первые шаги в генерации картинок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом тюториале мы получим основные графики, показанные на слайдах, а также натренируем Генеративно-состязательную модель на открытом наборе данных MNIST. Код написан с применением библиотеки PyTorch\n",
    "\n",
    "Тетрадка является выдержкой из курса \"Генеративные модели машинного обучения\" https://github.com/HSE-LAMBDA/DeepGenerativeModels/, авторы Денис Деркач, Максим Артемьев, Артём Рыжиков. Ревью тетрадки: Михаил Гущин."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи тюториала: \n",
    "1. Разобраться какая метрика лучше оценивает качество.\n",
    "2. Получить представление о проблемах метрик. \n",
    "3. Натренировать простейшую генеративно-состязательную сеть (JSGAN), посмотреть на её качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import distributions as distrs\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим вспомогательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Красиво рисует двумерное распределение\n",
    "def plot_2d_dots(dots, color='blue', label='None'):\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.scatter(dots[:, 0], dots[:, 1], s=1, c=color, label=label)\n",
    "\n",
    "def create_distr(mu, sigma):\n",
    "    return distrs.MultivariateNormal(mu, sigma)\n",
    "\n",
    "# Оборачивает параметры распределения в торчевские тензоры\n",
    "def get_parameters(mu=0., sigma=1.):\n",
    "    train_mu = torch.Tensor([mu, mu])\n",
    "    train_mu.requires_grad=True\n",
    "    train_sigma = torch.Tensor([[sigma, 0.0],\n",
    "                                [0.0, sigma]])\n",
    "    train_sigma.requires_grad=True\n",
    "    return train_mu, train_sigma\n",
    "\n",
    "def sample(d, num):\n",
    "    return d.sample(torch.Size([num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики качества\n",
    "\n",
    "##### Зададим 2D Гаусс как целевое распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.Tensor([-5, -5])\n",
    "sigma = torch.Tensor([[1., 0.0],\n",
    "                      [0.0, 1.]])\n",
    "\n",
    "target = create_distr(mu, sigma)\n",
    "# x - samples from the target distribution\n",
    "x = sample(target, 1000)\n",
    "# px = p(x) = probability of target samples for the target distribution\n",
    "px = target.log_prob(x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_2d_dots(x, color=px, label='target distr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аппроксимируем целевую функцию нашей, минимизируя KL дивергенцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting points\n",
    "train_mu, train_sigma = get_parameters()\n",
    "\n",
    "Q = create_distr(train_mu, train_sigma)\n",
    "q_sample = sample(Q, 1000)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_2d_dots(x, color='r', label='target distr')\n",
    "plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(qx, px):\n",
    "    # Clamp for the numerical stability \n",
    "    px, qx = px.clamp(min=1e-7), qx.clamp(min=1e-7)\n",
    "    return torch.mean(px * (px.log() - qx.log())) # YOUR CODE\n",
    "\n",
    "# train_mu and train_sigma are TRAINABLE parameters\n",
    "train_mu, train_sigma = get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try replacing SGD with Adam\n",
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=0.1)\n",
    "\n",
    "for i in range(5000):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    # qx = q(x) = probability of target samples for the train distribution\n",
    "    qx = Q.log_prob(x).exp()\n",
    "    loss = kl_loss(qx, px)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(x, color='r', label='target distr')\n",
    "        # q_sample - samples from the train distribution, just for visualization\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: всё довольно неплохо работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем с бимодальным распределением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = create_distr(torch.Tensor([-5, -5]), torch.Tensor([[1., 0.0], [0.0, 1.]]))\n",
    "target2 = create_distr(torch.Tensor([4, 3]), torch.Tensor([[1., 0.0], [0.0, 1.]]))\n",
    "\n",
    "x = torch.cat([sample(target1, 1000), sample(target2, 1000)])\n",
    "\n",
    "px = target1.log_prob(x).exp() + target2.log_prob(x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_2d_dots(x, color=px, label='target distr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters()\n",
    "\n",
    "Q = create_distr(train_mu, train_sigma)\n",
    "q_sample = sample(Q, 1000)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_2d_dots(x, color='r', label='target distr')\n",
    "plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=0.1)\n",
    "\n",
    "for i in range(5000):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    qx = Q.log_prob(x).exp()\n",
    "    loss = kl_loss(qx, px)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(x, color='r', label='target distr')\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: Распределение, которое получается минимизацией обратной KL дивергенции пытается покрыть оба пика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование дивергенции Йенсена-Шеннона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_div(qx, px):\n",
    "    return 0.5 * kl_loss(px, 0.5*px+0.5*qx) + 0.5 * kl_loss(qx, 0.5*px+0.5*qx) # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=0.1)\n",
    "\n",
    "for i in range(5000):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    qx = Q.log_prob(x).exp()\n",
    "    loss = js_div(qx, px)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'JS={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(x, color='r', label='target distr')\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: Распределение, которое получается минимизацией дивергенции ЙШ пытвается покрыть одну моду, но знает о второй."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Можно также посмотреть на другие метрики расстояния, например, MSE\n",
    "*as in [here](https://www.arxiv-vanity.com/papers/1611.04076/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSE_loss(qx, px):\n",
    "    return torch.nn.MSELoss()(qx, px) # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=0.5)\n",
    "\n",
    "for i in range(20000):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    qx = Q.log_prob(x).exp()\n",
    "    loss = LSE_loss(qx, px)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'LSE={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(x, color='r', label='target distr')\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample, color= Q.log_prob(q_sample).exp().detach(), label='train distr')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перейдём к тренировке простой Генеративно-состязательной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с подготовки. Мы используем comet_ml, позволяющий легко просматривать результаты. Больше информации можно найти в этом курсе https://stepik.org/course/60000/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as v_utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mnist_transforms = transforms.Compose([ # Compose combines a number of transforms into one operation\n",
    "    transforms.ToTensor(), # PIL Image -> Tensor\n",
    "    transforms.Normalize([0.5], [0.5]) # input = (input - 0.5) / 0.5 -> x \\sim input \\in [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use torchvision package to get MNIST dataset\n",
    "\n",
    "data_path = \"../data/\"\n",
    "\n",
    "train_dataset = datasets.MNIST(data_path,\n",
    "                               train=True,\n",
    "                               transform=mnist_transforms,\n",
    "                               target_transform=None,\n",
    "                               download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_loader))\n",
    "print(f'Label: {label[0]}')\n",
    "plt.imshow(img[0, 0, :,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша цель -- построить ГСС из двух разных сетей: генератора и дискриминатора.\n",
    "\n",
    "1. Генератор берёт шум из латентного пространства и выводит изображение (1x28x28). Цель состоит в том, чтобы \"обмануть\" Дискриминатор.\n",
    "2. Дискриминатор берёт изображение (1x28x28) и возвращает вероятность того, что изображение является реальным. Цель состоит в том, чтобы отличить реальные изображения от сгенерированных.\n",
    "\n",
    "Используя бинарную кросс-энтропию, мы минимизируем ЙШ расстояние между вещественным и \"сгенерированным\" распределением, сдвигая \"сгенерированные\" изображения ближе к вещественным. \n",
    "\n",
    "Оригинальная статья [here](https://www.arxiv-vanity.com/papers/1406.2661/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](GAN.png \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2,\n",
    "            mode='bilinear',\n",
    "            align_corners=True\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), # 16x7x7\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            self.upsample, # 16x14x14\n",
    "            nn.Conv2d(16, 32, 5, padding=2), # 32x14x14\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.LeakyReLU(),\n",
    "            self.upsample, # 32x28x28\n",
    "            nn.Conv2d(32, 32, 5, padding=2), # 32x28x28\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), # 32x28x28\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, 3, padding=1), # 1x28x28\n",
    "            nn.Tanh(), # 1x28x28 \\in [-1, 1]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 7, 7)\n",
    "        x = self.layers(x)\n",
    "        return self.final_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, stride=2, padding=3), # 16x14x14\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, 5, stride=2, padding=2), # 32x7x7\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), # 32x7x7\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1), # 1x7x7\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(1*7*7, 1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "generator = Generator().to(device)\n",
    "print(summary(generator, (7*7, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "print(summary(discriminator, (1, 28, 28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"lODeHEtCf7XLaV6DJrOfugNcA\",\n",
    "                        project_name=\"yandex-school-gan-mnist\", workspace=\"holybayes\")\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "def sample_noise(batch, dims, mean=0, std=0.1):\n",
    "    z = nn.init.normal_(torch.zeros(batch, dims, device=device), mean=mean, std=std)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_interval = 500\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc='Epoch loop'):\n",
    "    for iter_ind, (imgs, _) in tqdm(enumerate(train_loader), desc='Train loop', leave=False):\n",
    "        \n",
    "        ones = torch.ones(imgs.size(0), 1, device=device)\n",
    "        zeros = torch.zeros(imgs.size(0), 1, device=device)\n",
    "        step = epoch * len(train_loader) + iter_ind\n",
    "        \n",
    "        # generator update\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_imgs = generator(sample_noise(imgs.size(0), 7*7))\n",
    "        loss_G = criterion(discriminator(fake_imgs), ones)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # discriminator update\n",
    "        optimizer_D.zero_grad()\n",
    "        fake_imgs = generator(sample_noise(imgs.size(0), 7*7))\n",
    "        err_real = criterion(discriminator(imgs.to(device)), ones)\n",
    "        err_fake = criterion(discriminator(fake_imgs), zeros)\n",
    "        loss_D = err_real + err_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        experiment.log_metrics({'Generator loss': loss_G.item(),\n",
    "                                'Discriminator loss': loss_D.item()},\n",
    "                                epoch = epoch,\n",
    "                                step = step)        \n",
    "\n",
    "        if step % sample_interval == 0:\n",
    "            plt.figure(figsize = (10,10))\n",
    "\n",
    "            plt.title(\n",
    "                f\"[Epoch {epoch}/{n_epochs}]\" + \\\n",
    "                f\"[Batch {step%len(train_loader)}/{len(train_loader)}]\" + \\\n",
    "                f\"[D loss: {loss_D.item()}] [G loss: {loss_G.item()}]\"\n",
    "            )\n",
    "            \n",
    "            experiment.log_image(make_grid(fake_imgs.data[:25]).cpu().detach().numpy()[0, :, :])\n",
    "\n",
    "            plt.imshow(make_grid(fake_imgs.data[:25]).cpu().detach().numpy()[0, :, :])\n",
    "            experiment.log_figure()\n",
    "            plt.clf()\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *\n",
    "Some usefull [tricks](https://github.com/soumith/ganhacks/blob/master/README.md) for training GAN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN -- одна из самых популярных генеративных моделей на текущий момент. Она характеризуется:\n",
    "1. чёткими картинками (в противовес вариационным автокодировщикам);\n",
    "2. высокой гибкостью (мы можем делать всё более сильную модель генератора).\n",
    "\n",
    "При этом у неё есть и недостатки:\n",
    "1. коллапс мод (свойство метрики);\n",
    "2. трудности сходимости (особенно для классического GAN, например, затухающие градиенты);\n",
    "3. неявная функция финального распределения (мы можем хорошо сэмплировать, но функцию выписать трудно);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
